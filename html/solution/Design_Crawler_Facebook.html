<html>

<head>
   <link rel="stylesheet" type="text/css" href="../../css/utility.css">     <!-- external css file -->
</head>

<body>
<pre><code>

https://www.1point3acres.com/bbs/forum.php?mod=viewthread&tid=641063&extra=page%3D1%26filter%3Dsortid%26sortid%3D311%26searchoption%5B3086%5D%5Bvalue%5D%3D10%26searchoption%5B3086%5D%5Btype%5D%3Dradio%26searchoption%5B3087%5D%5Bvalue%5D%3D2%26searchoption%5B3087%5D%5Btype%5D%3Dradio%26searchoption%5B3046%5D%5Bvalue%5D%3D2%26searchoption%5B3046%5D%5Btype%5D%3Dradio%26sortid%3D311%26orderby%3Ddateline


假如你是一个黑客，你有10，000台电脑。带宽和cpu 都很高。但是node 之间的访问非常受限。
你从 某个根节点url 开始访问， 大概用过递归能得到10^9个url。你需要吧每个url的内容都下载下来，而且不能重复下载。
现在请你设计一个系统：
下载所有的网站， 不能重复下载， 尽量减少node之间的traffic。


</code></pre>
</body>
