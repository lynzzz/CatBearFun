<html>

<head>
   <link rel="stylesheet" type="text/css" href="../../css/utility.css">     <!-- external css file -->
</head>

<body>
<pre><code>
                                                      FB crawler design
这个题目的几种表达方法.

第一种表达方式:
假如你是一个黑客，你有10，000台电脑。带宽和cpu 都很高。但是node 之间的访问非常受限。
你从 某个根节点url 开始访问， 大概用过递归能得到10^9个url。你需要吧每个url的内容都下载下来，而且不能重复下载。
现在请你设计一个系统：
下载所有的网站， 不能重复下载， 尽量减少node之间的traffic。


https://www.1point3acres.com/bbs/forum.php?mod=viewthread&tid=641063&extra=page%3D1%26filter%3Dsortid%26sortid%3D311%26searchoption%5B3086%5D%5Bvalue%5D%3D10%26searchoption%5B3086%5D%5Btype%5D%3Dradio%26searchoption%5B3087%5D%5Bvalue%5D%3D2%26searchoption%5B3087%5D%5Btype%5D%3Dradio%26searchoption%5B3046%5D%5Bvalue%5D%3D2%26searchoption%5B3046%5D%5Btype%5D%3Dradio%26sortid%3D311%26orderby%3Ddateline




第2种表达方式:
We need to deploy the same software on each node. We have 10,000 nodes, the software can know about all the nodes. 
We have to minimize communication and make sure each node does equal amount of work. should be no sort of centralization of any kind


https://leetcode.com/discuss/interview-question/system-design/124657/Facebook-or-System-Design-or-A-web-crawler-that-will-crawl-Wikipedia




Questions:

What if one node fails or does not work?
Consistent hashing: if one node is done, the job would be done be the node on the right.


How do you know when the crawler is done?






Solution:
Hint: the software can know about all the nodes. => one node knows all nodes. It is a Peer to peer networking
Each node does the same amout of work: Consistent hashing:

Keyword:
Peer to Peer (P2P) DHT : Distributed Hash Table, 
Implementaion: Consistent hashing:



DHT 解决的问题. use a network of nodes to do the same the same thing. And node often joins and leaves 
the network.


Use consistant hashing as hashing algorithm to hash all node to a ring uniformally. (Hash by Ip?), then 
hash the url into the ring uniformally. Every node is responsible for the url on the left arc. 
Since hash algorithm is the same to all nodes, once one node crawl and get new urls, apply consistent hashing
algorithm to it, and figure out where it is in the ring and then figure out who is responsible for crawling it,
and then send this URL to that node. before send the url over, check if that node is alive, if not, send it to
its predesucssor, which is on the right of this node. The node needs to maintain its succussor and predesuccor.


problem is one node only knows all the URl it is crawling, other nodes do not know. What to do when it fails 
and it doesn't finish all the crawling.



Chord or KAD algorithm which is built on top of that, to see how to minimize communications and see how nodes collaborate.













                                                     Tradition Crawler design:
<li> <a href="https://acecodeinterview.com/web_crawler">Ace crawler design (讲的像科普知识,缺乏design价值)</a></li>
<li> <a href=https://www.educative.io/courses/grokking-the-system-design-interview/NE5LpPrWrKv
      >Grokkking crawler design（讲的比较详细</a></li>
      <li> <a href="https://medium.com/@morefree7/design-a-distributed-web-crawler-f67a8ebb8336"（Distributed Hashtable)</a></li>
            <li> <a href="https://link.springer.com/chapter/10.1007%2F978-3-540-24610-7_10" (Decentrialized Peer to Peer crawling network)</a></li>

      

I requirement + capacity

500M websites
Each website has 100 pages
Each page has average 100kb.
Need to crawl the entire internet every 2 weeks

Qps = 500M * 100 /(86400 * 14) = 41335
storage = 500M * 100 * 100kb = 5PB


<img src="../../img/crawler.png" width="800px" height="800px">
<img src="../../img/crawlerDetail.png" width="800px" height="800px">



II Core component:

1. URL frontier : 
      step 1: normalization
      step 2: filter(blacklist)
      step 3: has seen ?   (there is a URL DB check)
      step 4: prioritization. Put URL in queues with different priority. Front queue selector selects URL based on priority randomly. Make sure 
                              Hight priority queue gets severd and low priority queue not get starved.
                              
                              Put URL in different queue again and each queue has the same domain. back selector selects url from those queues based 
                              on a min priority queue that has the allowd visting time. t + deltaT. Each time it selects a url from a queue, it calculates the new 
                              time and insert it in min priority queue. Such that, it knows who is the next one to crawl and do not overload the website.
     
      step 5: Give the URL to fetcher

2. Fetcher (Async worker) 
      The component that actualy sends request to website. 
      step 1: Send HEAD request to url and check last modified header and see if it is different from last time
      Step 2: if yes, send GET request to url and get the html page.
      step 3: save the html in cache, storage and task queue 



3. Extractor (process HTML page)
      URL extractor and send it to url frontier 
      create index for searching purpose (Async)






Bottom neck: How to distribute URL to worker?
1. Simple Hash.
2. Optimization: based on location. 中国机器爬中国网站, 美国机器爬美国网站. 
3. Further optimization: Based on domain
    1台机器固定的爬多个Domian. 这样使得 Worker 可以借助本地优势 (Locality)，在内存中存储跟管辖域名相关的信息而不需要反复访问其他服务，比如存储域名爬取策略。 
    缺点是可能work load分配不均匀, 有些机器任务过重,有些机器没任务. 
    
    解决方法是consistent hashing, 但就不是hash by domain了


相似度分析?
1. Hashing. 不可靠
2. 近似算法. Jaccard index, cosine similarity



Database Schema:

Domain 
Domain Id
Domain name
last Crawl time


URL:
Url id
URl
last Crawl time
next Crawl time
gap 


Html is stored in big table(S3, Azure storage)


Politenss: robot.txt  Web site declariation file. Parse it before downloading anything.





                                   Groking website Question
Breadth-first or depth-first?  
BFS: 一次爬一个website的一个page, 慢,但是不影响网站正常traffic, But need to do three way handshake everytime establishing connection.
DFS: 一次性爬完一个website, 快,但是那个时间段可能会overload, 影响网站正常traffic


Scalability:

The URL frontier is the data structure that contains all the URLs that remain to be downloaded.
We start crawling using BFS of the web using seed url set. BFS requires a FIFO queue.


Assume there are 50B URls (2.5TB) and those cannot be saved in one server. we need to distribute URL in multiple machines.
using hash function URL domain -> server. The Downside is url is not uniformally distributed. 

In each server, There are multiple threads crawling the website and each thread is responsible for a FIFO sub queue.
We use another local hash function from URL domain-> Thread(Queue).   (一个Queue负责多个Domain)

The workflow is like One thread removes url from queue and crawls website,
gets more urls. Then It will apply hash function to first decide which machine is responsible for crawling the website and forwards
the url to that server, and then that server applies local hash function to decide which thread(queue) is responsible for crawling that website.
And then place the url in the sub queue that thread will crawl.

By doing this, at any given time, there is only one thread crawling the specified web site.
and by using FIFO queue, this website is not overloaded.


" Following politeness requirements must be kept in mind while designing a distributed URL frontier:

Our crawler should not overload a server by downloading a lot of pages from it.
We should not have multiple machines connecting a web server.

"


How to dedup html content?
Hash and generate 64bit checksum. 2^64  = 10^19 > 15B = 10^10 pages assumption. so 64bit check sum can represent all pages.

Size of checksum?
8byte * 15B = 1.2 *10^11 = 120GB. Those memory is distributed in different servers.

What if one serve do not have that many memory?
Save smaller data in LRU cache and everything in hard disk. the check flow is check LRU cache first and then
check hard disk. if not present, save the content and update hash.


DNS looks up is a big bottom neck. How to solve this? 
Cache DNS look up in the server and one server is responsible for crawling fixed website.


How to dedup URL content? 
4bytes checksum.
15B * 4byte = 60GB. Cache it in memory that can be used by all threads in one host.


A crawl of the entire Web takes weeks to complete. To guard against failures, our crawler can write regular snapshots of its state to the disk. 
An interrupted or aborted crawl can easily be restarted from the latest checkpoint.


7. Fault tolerance 
We should use consistent hashing to uniformally distribute url load to servers. If one server is down,
other server take over. 但是这样的话就有可能不同的crawl server同时Crawl同一个Website. 可能会overload website.


8. Data Partitioning
Each machine holds 3 set of data.
1. URL to be visited 
2. URL checksum 
3. Html checksum. 

It does checkpoint periodically and dumps the snapshot of the data it is holding to remote machine, such that 
when it is down, other machine can take over from last snapshot, since crawling takes weeks to finish.


9. Crawler Traps: To make crawler to crawl indefinitely.
purpose: 
a. Catch search engine crawler to boost their web rating.
b. Anti-spam traps are designed to catch crawlers used by spammers looking for email addresses,

</code></pre>
</body>

