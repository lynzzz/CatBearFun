<html>

<head>
   <link rel="stylesheet" type="text/css" href="../../css/utility.css">     <!-- external css file -->
</head>

<body>
<pre><code>

这个题目的几种表达方法.

第一种表达方式:
假如你是一个黑客，你有10，000台电脑。带宽和cpu 都很高。但是node 之间的访问非常受限。
你从 某个根节点url 开始访问， 大概用过递归能得到10^9个url。你需要吧每个url的内容都下载下来，而且不能重复下载。
现在请你设计一个系统：
下载所有的网站， 不能重复下载， 尽量减少node之间的traffic。


https://www.1point3acres.com/bbs/forum.php?mod=viewthread&tid=641063&extra=page%3D1%26filter%3Dsortid%26sortid%3D311%26searchoption%5B3086%5D%5Bvalue%5D%3D10%26searchoption%5B3086%5D%5Btype%5D%3Dradio%26searchoption%5B3087%5D%5Bvalue%5D%3D2%26searchoption%5B3087%5D%5Btype%5D%3Dradio%26searchoption%5B3046%5D%5Bvalue%5D%3D2%26searchoption%5B3046%5D%5Btype%5D%3Dradio%26sortid%3D311%26orderby%3Ddateline




第2种表达方式:
We need to deploy the same software on each node. We have 10,000 nodes, the software can know about all the nodes. 
We have to minimize communication and make sure each node does equal amount of work. should be no sort of centralization of any kind


https://leetcode.com/discuss/interview-question/system-design/124657/Facebook-or-System-Design-or-A-web-crawler-that-will-crawl-Wikipedia




Questions:

What if one node fails or does not work?
Consistent hashing: if one node is done, the job would be done be the node on the right.


How do you know when the crawler is done?






Solution:
Hint: the software can know about all the nodes. => one node knows all nodes. It is a Peer to peer networking
Each node does the same amout of work: Consistent hashing:

Keyword:
Peer to Peer (P2P) DHT : Distributed Hash Table, 
Implementaion: Consistent hashing:



DHT 解决的问题. use a network of nodes to do the same the same thing. And node often joins and leaves 
the network.


Use consistant hashing as hashing algorithm to hash all node to a ring uniformally. (Hash by Ip?), then 
hash the url into the ring uniformally. Every node is responsible for the url on the left arc. 
Since hash algorithm is the same to all nodes, once one node crawl and get new urls, apply consistent hashing
algorithm to it, and figure out where it is in the ring and then figure out who is responsible for crawling it,
and then send this URL to that node. before send the url over, check if that node is alive, if not, send it to
its predesucssor, which is on the right of this node. The node needs to maintain its succussor and predesuccor.


problem is one node only knows all the URl it is crawling, other nodes do not know. What to do when it fails 
and it doesn't finish all the crawling.



Chord or KAD algorithm which is built on top of that, to see how to minimize communications and see how nodes collaborate.








</code></pre>
</body>
