<html>

<head>
   <link rel="stylesheet" type="text/css" href="../../css/utility.css">     <!-- external css file -->
</head>

<body>
<pre><code>
                                                      FB crawler design
这个题目的几种表达方法.

第一种表达方式:
假如你是一个黑客，你有10，000台电脑。带宽和cpu 都很高。但是node 之间的访问非常受限。
你从 某个根节点url 开始访问， 大概用过递归能得到10^9个url。你需要吧每个url的内容都下载下来，而且不能重复下载。
现在请你设计一个系统：
下载所有的网站， 不能重复下载， 尽量减少node之间的traffic。


https://www.1point3acres.com/bbs/forum.php?mod=viewthread&tid=641063&extra=page%3D1%26filter%3Dsortid%26sortid%3D311%26searchoption%5B3086%5D%5Bvalue%5D%3D10%26searchoption%5B3086%5D%5Btype%5D%3Dradio%26searchoption%5B3087%5D%5Bvalue%5D%3D2%26searchoption%5B3087%5D%5Btype%5D%3Dradio%26searchoption%5B3046%5D%5Bvalue%5D%3D2%26searchoption%5B3046%5D%5Btype%5D%3Dradio%26sortid%3D311%26orderby%3Ddateline




第2种表达方式:
We need to deploy the same software on each node. We have 10,000 nodes, the software can know about all the nodes. 
We have to minimize communication and make sure each node does equal amount of work. should be no sort of centralization of any kind


https://leetcode.com/discuss/interview-question/system-design/124657/Facebook-or-System-Design-or-A-web-crawler-that-will-crawl-Wikipedia




Questions:

What if one node fails or does not work?
Consistent hashing: if one node is done, the job would be done be the node on the right.


How do you know when the crawler is done?






Solution:
Hint: the software can know about all the nodes. => one node knows all nodes. It is a Peer to peer networking
Each node does the same amout of work: Consistent hashing:

Keyword:
Peer to Peer (P2P) DHT : Distributed Hash Table, 
Implementaion: Consistent hashing:



DHT 解决的问题. use a network of nodes to do the same the same thing. And node often joins and leaves 
the network.


Use consistant hashing as hashing algorithm to hash all node to a ring uniformally. (Hash by Ip?), then 
hash the url into the ring uniformally. Every node is responsible for the url on the left arc. 
Since hash algorithm is the same to all nodes, once one node crawl and get new urls, apply consistent hashing
algorithm to it, and figure out where it is in the ring and then figure out who is responsible for crawling it,
and then send this URL to that node. before send the url over, check if that node is alive, if not, send it to
its predesucssor, which is on the right of this node. The node needs to maintain its succussor and predesuccor.


problem is one node only knows all the URl it is crawling, other nodes do not know. What to do when it fails 
and it doesn't finish all the crawling.



Chord or KAD algorithm which is built on top of that, to see how to minimize communications and see how nodes collaborate.













                                                     Tradition Crawler design:
<li> <a href="https://acecodeinterview.com/web_crawler">Ace crawler design (设计偏重于high level)</a></li>

                                                     

I requirement + capacity

500M websites
Each website has 100 pages
Each page has average 100kb.
Need to crawl the entire internet every 2 weeks

Qps = 500M * 100 /(86400 * 14) = 41335
storage = 500M * 100 * 100kb = 5PB



II Core component:

1. URL frontier : 
      step 1: normalization
      step 2: filter 
      step 3: has seen ? 
      step 4: prioritization. Put URL in queues with different priority. Front queue selector selects URL based on priority randomly. Make sure 
                              Hight priority queue gets severd and low priority queue not get starved.
                              
                              Put URL in different queue again and each queue has the same domain. back selector selects url from those queues based 
                              on a min priority queue that has the allowd visting time. t + deltaT. Each time it selects a url from a queue, it calculates the new 
                              time and insert it in min priority queue. Such that, it knows who is the next one to crawl and do not overload the website.
     
      step 5: Give the URL to fetcher

2. Fetcher (Async worker) 
      The component that actualy sends request to website. 
      step 1: Send HEAD request to url and check last modified header and see if it is different from last time
      Step 2: if yes, send GET request to url and get the html page.
      step 3: save the html in cache, storage and task queue 



3. Task Queue (process HTML page)
      URL extractor and send it to url frontier 
      create index for searching purpose (Async)








</code></pre>
</body>
